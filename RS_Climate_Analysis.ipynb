{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76a2aaf",
   "metadata": {},
   "source": [
    "# Análise das Mudanças Climáticas no Rio Grande do Sul\n",
    "Este notebook executa o pipeline completo para investigar mudanças sazonais, variabilidade e extremos de temperatura no Rio Grande do Sul (RS) usando os dados do **BDMEP/INMET**.\n",
    "\n",
    "**Fluxo geral:**\n",
    "1. Configuração do ambiente e parâmetros.\n",
    "2. Leitura automática dos arquivos CSV das estações.\n",
    "3. Pré‑processamento e agregação diária/mensal/sazonal.\n",
    "4. Cálculo de indicadores (médias, desvios, extremos, anomalias).\n",
    "5. Análise de tendências (regressão linear, Mann‑Kendall).\n",
    "6. Visualizações (séries temporais, boxplots, mapas de anomalia).\n",
    "\n",
    "> **Dica:** copie seus CSVs para a pasta `./dados` antes de executar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "499555d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pymannkendall as mk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Garante que os gráficos apareçam no notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8341cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta de dados: /home/vitorklock/visualStudio/projects/study/unijui/PI/2025/clima-rs/dados\n",
      "Pasta de saídas: /home/vitorklock/visualStudio/projects/study/unijui/PI/2025/clima-rs/outputs\n"
     ]
    }
   ],
   "source": [
    "# --- Configurações do usuário ---\n",
    "DATA_DIR = Path('./dados')          # Pasta com os CSVs BDMEP\n",
    "SHAPE_PATH = Path('./RS.shp')       # Shapefile do contorno do RS\n",
    "OUT_DIR = Path('./outputs')         # Pasta de saída\n",
    "BASELINE_START = 1961               # Início do período de referência\n",
    "BASELINE_END   = 1990               # Fim do período de referência\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "print(f'Pasta de dados: {DATA_DIR.resolve()}')\n",
    "print(f'Pasta de saídas: {OUT_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8afe5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bdmepr_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega um CSV do BDMEP (versão 5 - Corrigida com os cabeçalhos do usuário).\n",
    "    Usa a dica da \"linha dupla\" para separar metadados e dados, e usa os nomes\n",
    "    corretos de colunas fornecidos pelo usuário.\n",
    "    \"\"\"\n",
    "    # --- Passo 1: Ler o arquivo e separar metadados e dados ---\n",
    "    try:\n",
    "        raw_content = path.read_text(encoding='latin1', errors='ignore')\n",
    "    except UnicodeDecodeError:\n",
    "        raw_content = path.read_text(encoding='utf-8', errors='ignore')\n",
    "\n",
    "    parts = raw_content.replace('\\r\\n', '\\n').split('\\n\\n', 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Não foi possível encontrar o separador de linha dupla no arquivo: {path.name}\")\n",
    "    metadata_block, data_block = parts\n",
    "    \n",
    "    # --- Passo 2: Extrair metadados ---\n",
    "    metadata = {}\n",
    "    for line in metadata_block.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            metadata[key.strip().upper()] = value.strip()\n",
    "\n",
    "    # --- Passo 3: Ler os dados tabulares CORRETAMENTE ---\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO(data_block),\n",
    "        sep=';',\n",
    "        decimal=',',\n",
    "        # Trata 'null', '-9999', etc. como valores Nulos/NaN\n",
    "        na_values=['null', '-9999', '9999.9'],\n",
    "        low_memory=False\n",
    "    )\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # --- Passo 4: Renomear colunas com o mapeamento CORRETO ---\n",
    "    rename_map = {\n",
    "        # ⇢ grafias com vírgula:\n",
    "        'Data Medicao': 'date_str',\n",
    "        'PRECIPITACAO TOTAL, DIARIO (AUT)(mm)': 'precip',\n",
    "        'TEMPERATURA MEDIA, DIARIA (AUT)(°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA, DIARIA (AUT)(°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA, DIARIA (AUT)(°C)': 'tmin',\n",
    "\n",
    "        # ⇢ grafias SEM vírgula:\n",
    "        'TEMPERATURA MEDIA DIARIA (AUT)(°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA DIARIA (AUT)(°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA DIARIA (AUT)(°C)': 'tmin',\n",
    "        'PRECIPITACAO TOTAL DIARIA (AUT)(mm)': 'precip',\n",
    "\n",
    "        # ⇢ outras variações comuns encontradas em séries antigas:\n",
    "        'TEMPERATURA MEDIA DIARIA (°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA DIARIA (°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA DIARIA (°C)': 'tmin',\n",
    "        'TEMPERATURA MEDIA COMPENSADA (°C)': 'tmedia',\n",
    "        'PRECIPITACAO TOTAL, DIARIA (mm)': 'precip',\n",
    "        'DATA': 'date_str', 'Data': 'date_str',\n",
    "    }\n",
    "    # Renomeia as colunas encontradas no dicionário\n",
    "    df = df.rename(columns=lambda c: rename_map.get(c.strip(), c.strip()))\n",
    "    \n",
    "    # --- Passo 5: Processar data e variáveis ---\n",
    "    # Se a coluna de data não foi renomeada, assume que é a primeira\n",
    "    if 'date_str' not in df.columns and len(df.columns) > 0:\n",
    "        df = df.rename(columns={df.columns[0]: 'date_str'})\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date_str'], errors='coerce')\n",
    "    if df['date'].isna().all(): raise ValueError(f\"Não foi possível converter as datas para DateTime no arquivo {path.name}\")\n",
    "    df = df.set_index('date').sort_index()\n",
    "\n",
    "    # LÓGICA CHAVE: Calcular tmedia se não existir\n",
    "    if 'tmedia' not in df.columns and 'tmax' in df.columns and 'tmin' in df.columns:\n",
    "        df['tmedia'] = (df['tmax'] + df['tmin']) / 2\n",
    "        \n",
    "    # Garante que as colunas são numéricas e aplica limites físicos\n",
    "    final_cols = ['tmedia', 'tmax', 'tmin', 'precip']\n",
    "    for col in final_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            if col.startswith('t'):\n",
    "                df[col] = df[col].where((df[col] > -50) & (df[col] < 50))\n",
    "            elif col == 'precip':\n",
    "                df[col] = df[col].where(df[col] >= 0)\n",
    "\n",
    "    # --- Passo 6: Guardar metadados e retornar ---\n",
    "    df.attrs['station_code'] = metadata.get('CODIGO OMM', metadata.get('CODIGO', path.stem.split('_')[1]))\n",
    "    df.attrs['latitude'] = pd.to_numeric(metadata.get('LATITUDE'), errors='coerce')\n",
    "    df.attrs['longitude'] = pd.to_numeric(metadata.get('LONGITUDE'), errors='coerce')\n",
    "    df.attrs['filename'] = path.name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf689c0",
   "metadata": {},
   "source": [
    "# --- Bloco de Execução Principal ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730e8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lendo estações: 100%|██████████| 43/43 [00:00<00:00, 91.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "43 estações carregadas com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = list(DATA_DIR.glob('*.csv'))\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo CSV encontrado em {DATA_DIR}\")\n",
    "\n",
    "stations_meta = []        # onde vamos armazenar as coordenadas etc.\n",
    "data_dict      = {}       # df diários completos por estação\n",
    "failed_files   = []\n",
    "\n",
    "for f in tqdm(all_files, desc='Lendo estações'):\n",
    "    try:\n",
    "        df_station = load_bdmepr_csv(f)\n",
    "    \n",
    "        code = df_station.attrs['station_code']\n",
    "        data_dict[code] = df_station\n",
    "\n",
    "        stations_meta.append({\n",
    "            'code'     : code,\n",
    "            'name'     : df_station.attrs.get('station_name', f.stem.split('_')[0]),\n",
    "            'lat'      : df_station.attrs['latitude'],\n",
    "            'lon'      : df_station.attrs['longitude'],\n",
    "            'alt'      : df_station.attrs.get('altitude', np.nan),\n",
    "            'data_ini' : df_station.index.min(),\n",
    "            'data_fim' : df_station.index.max(),\n",
    "        })\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_files.append((f.name, str(e)))\n",
    "        continue\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nAVISO: Alguns arquivos não puderam ser carregados:\")\n",
    "    for fname, error in failed_files:\n",
    "        print(f\"- {fname}: {error}\")\n",
    "\n",
    "print(f'\\n{len(data_dict)} estações carregadas com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c06037",
   "metadata": {},
   "source": [
    "## 2. GeoDataFrame de metadados (para mapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ad5c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   code   name        lat        lon  alt   data_ini   data_fim  \\\n",
      "0  A854  dados -27.395556 -53.429444  NaN 2007-12-13 2025-01-01   \n",
      "1  A844  dados -28.222381 -51.512845  NaN 2007-03-01 2025-01-01   \n",
      "2  A833  dados -29.191599 -54.885653  NaN 2009-02-03 2025-01-01   \n",
      "3  A831  dados -30.368611 -56.437222  NaN 2007-10-16 2025-01-01   \n",
      "4  A827  dados -31.347778 -54.013333  NaN 2007-01-03 2025-01-01   \n",
      "\n",
      "                      geometry  \n",
      "0  POINT (-53.42944 -27.39556)  \n",
      "1  POINT (-51.51284 -28.22238)  \n",
      "2   POINT (-54.88565 -29.1916)  \n",
      "3  POINT (-56.43722 -30.36861)  \n",
      "4  POINT (-54.01333 -31.34778)  \n"
     ]
    }
   ],
   "source": [
    "meta_df = pd.DataFrame(stations_meta).dropna(subset=['lat', 'lon'])\n",
    "gdf_meta = gpd.GeoDataFrame(\n",
    "    meta_df,\n",
    "    geometry=gpd.points_from_xy(meta_df.lon, meta_df.lat),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(gdf_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e28434",
   "metadata": {},
   "source": [
    "## 3. Funções auxiliares: estação do ano, climatologia-base e agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def add_southern_season(df):\n",
    "    \"\"\"Adiciona colunas de ano-mês-estação (hemisfério sul).\"\"\"\n",
    "    d = df.copy()\n",
    "    d['year']  = d.index.year\n",
    "    d['month'] = d.index.month\n",
    "\n",
    "    m2s = {12:'summer',1:'summer',2:'summer',\n",
    "            3:'autumn',4:'autumn',5:'autumn',\n",
    "            6:'winter',7:'winter',8:'winter',\n",
    "            9:'spring',10:'spring',11:'spring'}\n",
    "    d['season'] = d['month'].map(m2s)\n",
    "    d.loc[d['month']==12, 'year'] += 1       # dez pertence ao verão do ano seguinte\n",
    "    return d\n",
    "\n",
    "def seasonal_summary(df):\n",
    "    \"\"\"\n",
    "    Gera quadro de métricas sazonais **apenas** para variáveis presentes.\n",
    "    - tmedia_mean / tmedia_sd\n",
    "    - tmax_90p   : nº de dias acima do percentil 90 da própria série\n",
    "    - tmin_10p   : nº de dias abaixo do percentil 10\n",
    "    - precip_sum : soma da precipitação\n",
    "    \"\"\"\n",
    "    d = add_southern_season(df)\n",
    "\n",
    "    agg = {}\n",
    "    if 'tmedia' in d.columns:\n",
    "        agg['tmedia_mean'] = ('tmedia', 'mean')\n",
    "        agg['tmedia_sd'  ] = ('tmedia', 'std')\n",
    "    if 'tmax' in d.columns:\n",
    "        agg['tmax_90p'] = ('tmax',  lambda s: (s > s.quantile(0.90)).sum())\n",
    "    if 'tmin' in d.columns:\n",
    "        agg['tmin_10p'] = ('tmin',  lambda s: (s < s.quantile(0.10)).sum())\n",
    "    if 'precip' in d.columns:\n",
    "        agg['precip_sum'] = ('precip','sum')\n",
    "\n",
    "    return (\n",
    "        d.groupby(['year','season'])\n",
    "         .agg(**agg)\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "def climatology_base(seasonal, start=1961, end=1990):\n",
    "    \"\"\"\n",
    "    Climatologia 1961-1990 da temperatura média.\n",
    "    Se 'tmedia_mean' não existir (ou não houver anos dentro do intervalo),\n",
    "    devolve apenas a lista de estações com NaN — assim o merge não quebra.\n",
    "    \"\"\"\n",
    "    if 'tmedia_mean' not in seasonal.columns:\n",
    "        # devolve seasons únicas com NaN\n",
    "        return (seasonal[['season']]\n",
    "                .drop_duplicates()\n",
    "                .assign(clim_tmedia=np.nan))\n",
    "\n",
    "    base = seasonal.query(\"@start <= year <= @end\").dropna(subset=['tmedia_mean'])\n",
    "    if base.empty:\n",
    "        return (seasonal[['season']]\n",
    "                .drop_duplicates()\n",
    "                .assign(clim_tmedia=np.nan))\n",
    "\n",
    "    return (\n",
    "        base.groupby('season')\n",
    "            .agg(clim_tmedia=('tmedia_mean','mean'))\n",
    "            .reset_index()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff863f",
   "metadata": {},
   "source": [
    "## 4. Calcula agregados e anomalias para cada estação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca6a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando sazonais: 100%|██████████| 43/43 [00:00<00:00, 123.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# %% Calculo das métricas sazonais + anomalias\n",
    "seasonal_dict = {}\n",
    "anomaly_dict  = {}\n",
    "\n",
    "for code, df in tqdm(data_dict.items(), desc='Calculando sazonais'):\n",
    "    seas = seasonal_summary(df)                       # métricas da estação\n",
    "    clim = climatology_base(seas, BASELINE_START, BASELINE_END)\n",
    "\n",
    "    # junta climatologia (pode vir SEM 'clim_tmedia' se não houver tmedia)\n",
    "    seas = seas.merge(clim, on='season', how='left')\n",
    "\n",
    "    # calcula anomalia só se ambas existirem\n",
    "    if {'tmedia_mean', 'clim_tmedia'}.issubset(seas.columns):\n",
    "        seas['tmedia_anom'] = seas['tmedia_mean'] - seas['clim_tmedia']\n",
    "    else:\n",
    "        seas['tmedia_anom'] = np.nan                  # placeholder\n",
    "\n",
    "    seasonal_dict[code] = seas\n",
    "    anomaly_dict[code]  = seas[['year', 'season', 'tmedia_anom']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6582aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 110})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ff27962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatology_base(seasonal: pd.DataFrame, start: int | None = 1961,\n",
    "                      end: int | None = 1990, *, fallback_years: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Retorna a climatologia (média  da *tmedia_mean*) por estação do ano.\n",
    "\n",
    "    - Se houver pelo menos 5 anos dentro do intervalo *start‑end* ➜ usa‑se esse\n",
    "      período oficial.\n",
    "    - Caso contrário, calcula‑se a média dos **primeiros `fallback_years` anos\n",
    "      disponíveis** (mínimo de 5 anos também).  Se ainda assim não houver dados\n",
    "      suficientes, devolve‐se *NaN*.\n",
    "    \"\"\"\n",
    "    if 'tmedia_mean' not in seasonal.columns:\n",
    "        return (seasonal[['season']].drop_duplicates().assign(clim_tmedia=pd.NA))\n",
    "\n",
    "    # ── 1. Tentativa com período oficial ──────────────────────────────────────\n",
    "    if start is not None and end is not None:\n",
    "        official = seasonal.query(\"@start <= year <= @end and tmedia_mean.notna()\")\n",
    "        if official['year'].nunique() >= 5:\n",
    "            return (official.groupby('season')\n",
    "                           .agg(clim_tmedia=('tmedia_mean', 'mean'))\n",
    "                           .reset_index())\n",
    "\n",
    "    # ── 2. Fallback: primeiros N anos disponíveis ────────────────────────────\n",
    "    min_year = seasonal['year'].min()\n",
    "    max_fallback = min_year + fallback_years - 1\n",
    "    fallback_df = seasonal.query(\"@min_year <= year <= @max_fallback and tmedia_mean.notna()\")\n",
    "    if fallback_df['year'].nunique() >= 5:\n",
    "        return (fallback_df.groupby('season')\n",
    "                          .agg(clim_tmedia=('tmedia_mean', 'mean'))\n",
    "                          .reset_index())\n",
    "\n",
    "    # ── 3. Sem dados suficientes ─────────────────────────────────────────────\n",
    "    return (seasonal[['season']].drop_duplicates().assign(clim_tmedia=pd.NA))\n",
    "\n",
    "\n",
    "\n",
    "def plot_rs_mean_anomaly(seasonal_dict: dict[str, pd.DataFrame], *, save: bool = False):\n",
    "    \"\"\"Plota a anomalia média estadual por estação do ano ao longo dos anos.\"\"\"\n",
    "    frames = [df[['year', 'season', 'tmedia_anom']].dropna() for df in seasonal_dict.values()\n",
    "              if 'tmedia_anom' in df.columns]\n",
    "\n",
    "    if not frames:\n",
    "        print(\"⚠️  Nenhuma anomalia calculada. Verifique as séries ou o cálculo da climatologia.\")\n",
    "        return\n",
    "\n",
    "    all_anom = pd.concat(frames, ignore_index=True)\n",
    "    if all_anom.empty:\n",
    "        print(\"⚠️  DataFrame de anomalias ficou vazio após concatenação.\")\n",
    "        return\n",
    "\n",
    "    mean_anom = (all_anom.groupby(['year', 'season'])['tmedia_anom']\n",
    "                        .mean().unstack().sort_index())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    mean_anom.plot(ax=ax, marker=\"o\")\n",
    "    ax.axhline(0, color=\"gray\", lw=0.8)\n",
    "    ax.set_ylabel(\"Anomalia de T média (°C)\")\n",
    "    ax.set_xlabel(\"Ano\")\n",
    "    ax.set_title(\"Anomalia média estadual da temperatura por estação do ano\")\n",
    "    ax.legend(title=\"Estação\")\n",
    "    ax.grid(True, ls=\":\", lw=0.5)\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(OUT_DIR / \"serie_anomalia_media_RS.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b57681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  DataFrame de anomalias ficou vazio após concatenação.\n"
     ]
    }
   ],
   "source": [
    "plot_rs_mean_anomaly(seasonal_dict, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "393e989d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tmedia_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/clima-rs/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tmedia_mean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     clim \u001b[38;5;241m=\u001b[39m climatology_base(seas, BASELINE_START, BASELINE_END, fallback_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m     seas \u001b[38;5;241m=\u001b[39m seas\u001b[38;5;241m.\u001b[39mmerge(clim, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     seas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmedia_anom\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mseas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtmedia_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m seas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclim_tmedia\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m     seasonal_dict[code] \u001b[38;5;241m=\u001b[39m seas\n\u001b[1;32m      9\u001b[0m anomaly_dict \u001b[38;5;241m=\u001b[39m {c: d[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmedia_anom\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m c,d \u001b[38;5;129;01min\u001b[39;00m seasonal_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.virtualenvs/clima-rs/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.virtualenvs/clima-rs/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tmedia_mean'"
     ]
    }
   ],
   "source": [
    "seasonal_dict = {}\n",
    "for code, df in data_dict.items():\n",
    "    seas = seasonal_summary(df)\n",
    "    clim = climatology_base(seas, BASELINE_START, BASELINE_END, fallback_years=10)\n",
    "    seas = seas.merge(clim, on='season', how='left')\n",
    "    seas['tmedia_anom'] = seas['tmedia_mean'] - seas['clim_tmedia']\n",
    "    seasonal_dict[code] = seas\n",
    "\n",
    "anomaly_dict = {c: d[['year','season','tmedia_anom']] for c,d in seasonal_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca804b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_recent_mean_anomaly(seasonal_dict, gdf_meta, SHAPE_PATH, start_year=2015, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clima-rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
