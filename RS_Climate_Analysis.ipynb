{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76a2aaf",
   "metadata": {},
   "source": [
    "# Análise das Mudanças Climáticas no Rio Grande do Sul\n",
    "Este notebook executa o pipeline completo para investigar mudanças sazonais, variabilidade e extremos de temperatura no Rio Grande do Sul (RS) usando os dados do **BDMEP/INMET**.\n",
    "\n",
    "**Fluxo geral:**\n",
    "1. Configuração do ambiente e parâmetros.\n",
    "2. Leitura automática dos arquivos CSV das estações.\n",
    "3. Pré‑processamento e agregação diária/mensal/sazonal.\n",
    "4. Cálculo de indicadores (médias, desvios, extremos, anomalias).\n",
    "5. Análise de tendências (regressão linear, Mann‑Kendall).\n",
    "6. Visualizações (séries temporais, boxplots, mapas de anomalia).\n",
    "\n",
    "> **Dica:** copie seus CSVs para a pasta `./dados` antes de executar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499555d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pymannkendall as mk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import io\n",
    "\n",
    "# Garante que os gráficos apareçam no notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8341cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta de dados: /home/vitorklock/visualStudio/projects/study/unijui/PI/2025/clima-rs/dados\n",
      "Pasta de saídas: /home/vitorklock/visualStudio/projects/study/unijui/PI/2025/clima-rs/outputs\n"
     ]
    }
   ],
   "source": [
    "# --- Configurações do usuário ---\n",
    "DATA_DIR = Path('./dados')          # Pasta com os CSVs BDMEP\n",
    "SHAPE_PATH = Path('./RS.shp')       # Shapefile do contorno do RS\n",
    "OUT_DIR = Path('./outputs')         # Pasta de saída\n",
    "BASELINE_START = 1961               # Início do período de referência\n",
    "BASELINE_END   = 1990               # Fim do período de referência\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "print(f'Pasta de dados: {DATA_DIR.resolve()}')\n",
    "print(f'Pasta de saídas: {OUT_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afe5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bdmepr_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega um CSV do BDMEP (versão 5 - Corrigida com os cabeçalhos do usuário).\n",
    "    Usa a dica da \"linha dupla\" para separar metadados e dados, e usa os nomes\n",
    "    corretos de colunas fornecidos pelo usuário.\n",
    "    \"\"\"\n",
    "    # --- Passo 1: Ler o arquivo e separar metadados e dados ---\n",
    "    try:\n",
    "        raw_content = path.read_text(encoding='latin1', errors='ignore')\n",
    "    except UnicodeDecodeError:\n",
    "        raw_content = path.read_text(encoding='utf-8', errors='ignore')\n",
    "\n",
    "    parts = raw_content.replace('\\r\\n', '\\n').split('\\n\\n', 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Não foi possível encontrar o separador de linha dupla no arquivo: {path.name}\")\n",
    "    metadata_block, data_block = parts\n",
    "    \n",
    "    # --- Passo 2: Extrair metadados ---\n",
    "    metadata = {}\n",
    "    for line in metadata_block.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            metadata[key.strip().upper()] = value.strip()\n",
    "\n",
    "    # --- Passo 3: Ler os dados tabulares CORRETAMENTE ---\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO(data_block),\n",
    "        sep=';',\n",
    "        decimal=',',\n",
    "        # Trata 'null', '-9999', etc. como valores Nulos/NaN\n",
    "        na_values=['null', '-9999', '9999.9'],\n",
    "        low_memory=False\n",
    "    )\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # --- Passo 4: Renomear colunas com o mapeamento CORRETO ---\n",
    "    rename_map = {\n",
    "        # ⇢ grafias com vírgula:\n",
    "        'Data Medicao': 'date_str',\n",
    "        'PRECIPITACAO TOTAL, DIARIO (AUT)(mm)': 'precip',\n",
    "        'TEMPERATURA MEDIA, DIARIA (AUT)(°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA, DIARIA (AUT)(°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA, DIARIA (AUT)(°C)': 'tmin',\n",
    "\n",
    "        # ⇢ grafias SEM vírgula:\n",
    "        'TEMPERATURA MEDIA DIARIA (AUT)(°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA DIARIA (AUT)(°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA DIARIA (AUT)(°C)': 'tmin',\n",
    "        'PRECIPITACAO TOTAL DIARIA (AUT)(mm)': 'precip',\n",
    "\n",
    "        # ⇢ outras variações comuns encontradas em séries antigas:\n",
    "        'TEMPERATURA MEDIA DIARIA (°C)': 'tmedia',\n",
    "        'TEMPERATURA MAXIMA DIARIA (°C)': 'tmax',\n",
    "        'TEMPERATURA MINIMA DIARIA (°C)': 'tmin',\n",
    "        'TEMPERATURA MEDIA COMPENSADA (°C)': 'tmedia',\n",
    "        'PRECIPITACAO TOTAL, DIARIA (mm)': 'precip',\n",
    "        'DATA': 'date_str', 'Data': 'date_str',\n",
    "    }\n",
    "    # Renomeia as colunas encontradas no dicionário\n",
    "    df = df.rename(columns=lambda c: rename_map.get(c.strip(), c.strip()))\n",
    "    \n",
    "    # --- Passo 5: Processar data e variáveis ---\n",
    "    # Se a coluna de data não foi renomeada, assume que é a primeira\n",
    "    if 'date_str' not in df.columns and len(df.columns) > 0:\n",
    "        df = df.rename(columns={df.columns[0]: 'date_str'})\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date_str'], errors='coerce')\n",
    "    if df['date'].isna().all(): raise ValueError(f\"Não foi possível converter as datas para DateTime no arquivo {path.name}\")\n",
    "    df = df.set_index('date').sort_index()\n",
    "\n",
    "    # LÓGICA CHAVE: Calcular tmedia se não existir\n",
    "    if 'tmedia' not in df.columns and 'tmax' in df.columns and 'tmin' in df.columns:\n",
    "        df['tmedia'] = (df['tmax'] + df['tmin']) / 2\n",
    "        \n",
    "    # Garante que as colunas são numéricas e aplica limites físicos\n",
    "    final_cols = ['tmedia', 'tmax', 'tmin', 'precip']\n",
    "    for col in final_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            if col.startswith('t'):\n",
    "                df[col] = df[col].where((df[col] > -50) & (df[col] < 50))\n",
    "            elif col == 'precip':\n",
    "                df[col] = df[col].where(df[col] >= 0)\n",
    "\n",
    "    # --- Passo 6: Guardar metadados e retornar ---\n",
    "    df.attrs['station_code'] = metadata.get('CODIGO OMM', metadata.get('CODIGO', path.stem.split('_')[1]))\n",
    "    df.attrs['latitude'] = pd.to_numeric(metadata.get('LATITUDE'), errors='coerce')\n",
    "    df.attrs['longitude'] = pd.to_numeric(metadata.get('LONGITUDE'), errors='coerce')\n",
    "    df.attrs['filename'] = path.name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf689c0",
   "metadata": {},
   "source": [
    "# --- Bloco de Execução Principal ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730e8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lendo estações: 100%|██████████| 43/43 [00:00<00:00, 104.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "43 estações carregadas com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = list(DATA_DIR.glob('*.csv'))\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo CSV encontrado em {DATA_DIR}\")\n",
    "\n",
    "stations_meta = []        # onde vamos armazenar as coordenadas etc.\n",
    "data_dict      = {}       # df diários completos por estação\n",
    "failed_files   = []\n",
    "\n",
    "for f in tqdm(all_files, desc='Lendo estações'):\n",
    "    try:\n",
    "        df_station = load_bdmepr_csv(f)\n",
    "    \n",
    "        code = df_station.attrs['station_code']\n",
    "        data_dict[code] = df_station\n",
    "\n",
    "        stations_meta.append({\n",
    "            'code'     : code,\n",
    "            'name'     : df_station.attrs.get('station_name', f.stem.split('_')[0]),\n",
    "            'lat'      : df_station.attrs['latitude'],\n",
    "            'lon'      : df_station.attrs['longitude'],\n",
    "            'alt'      : df_station.attrs.get('altitude', np.nan),\n",
    "            'data_ini' : df_station.index.min(),\n",
    "            'data_fim' : df_station.index.max(),\n",
    "        })\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_files.append((f.name, str(e)))\n",
    "        continue\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nAVISO: Alguns arquivos não puderam ser carregados:\")\n",
    "    for fname, error in failed_files:\n",
    "        print(f\"- {fname}: {error}\")\n",
    "\n",
    "print(f'\\n{len(data_dict)} estações carregadas com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c06037",
   "metadata": {},
   "source": [
    "## 2. GeoDataFrame de metadados (para mapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad5c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   code   name        lat        lon  alt   data_ini   data_fim  \\\n",
      "0  A854  dados -27.395556 -53.429444  NaN 2007-12-13 2025-01-01   \n",
      "1  A844  dados -28.222381 -51.512845  NaN 2007-03-01 2025-01-01   \n",
      "2  A833  dados -29.191599 -54.885653  NaN 2009-02-03 2025-01-01   \n",
      "3  A831  dados -30.368611 -56.437222  NaN 2007-10-16 2025-01-01   \n",
      "4  A827  dados -31.347778 -54.013333  NaN 2007-01-03 2025-01-01   \n",
      "\n",
      "                      geometry  \n",
      "0  POINT (-53.42944 -27.39556)  \n",
      "1  POINT (-51.51284 -28.22238)  \n",
      "2   POINT (-54.88565 -29.1916)  \n",
      "3  POINT (-56.43722 -30.36861)  \n",
      "4  POINT (-54.01333 -31.34778)  \n"
     ]
    }
   ],
   "source": [
    "meta_df = pd.DataFrame(stations_meta).dropna(subset=['lat', 'lon'])\n",
    "gdf_meta = gpd.GeoDataFrame(\n",
    "    meta_df,\n",
    "    geometry=gpd.points_from_xy(meta_df.lon, meta_df.lat),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(gdf_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e28434",
   "metadata": {},
   "source": [
    "## 3. Funções auxiliares: estação do ano, climatologia-base e agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def add_southern_season(df):\n",
    "    \"\"\"Adiciona colunas de ano-mês-estação (hemisfério sul).\"\"\"\n",
    "    d = df.copy()\n",
    "    d['year']  = d.index.year\n",
    "    d['month'] = d.index.month\n",
    "\n",
    "    m2s = {12:'summer',1:'summer',2:'summer',\n",
    "            3:'autumn',4:'autumn',5:'autumn',\n",
    "            6:'winter',7:'winter',8:'winter',\n",
    "            9:'spring',10:'spring',11:'spring'}\n",
    "    d['season'] = d['month'].map(m2s)\n",
    "    d.loc[d['month']==12, 'year'] += 1       # dez pertence ao verão do ano seguinte\n",
    "    return d\n",
    "\n",
    "def seasonal_summary(df):\n",
    "    \"\"\"\n",
    "    Gera quadro de métricas sazonais **apenas** para variáveis presentes.\n",
    "    - tmedia_mean / tmedia_sd\n",
    "    - tmax_90p   : nº de dias acima do percentil 90 da própria série\n",
    "    - tmin_10p   : nº de dias abaixo do percentil 10\n",
    "    - precip_sum : soma da precipitação\n",
    "    \"\"\"\n",
    "    d = add_southern_season(df)\n",
    "\n",
    "    agg = {}\n",
    "    if 'tmedia' in d.columns:\n",
    "        agg['tmedia_mean'] = ('tmedia', 'mean')\n",
    "        agg['tmedia_sd'  ] = ('tmedia', 'std')\n",
    "    if 'tmax' in d.columns:\n",
    "        agg['tmax_90p'] = ('tmax',  lambda s: (s > s.quantile(0.90)).sum())\n",
    "    if 'tmin' in d.columns:\n",
    "        agg['tmin_10p'] = ('tmin',  lambda s: (s < s.quantile(0.10)).sum())\n",
    "    if 'precip' in d.columns:\n",
    "        agg['precip_sum'] = ('precip','sum')\n",
    "\n",
    "    return (\n",
    "        d.groupby(['year','season'])\n",
    "         .agg(**agg)\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "def climatology_base(seasonal, start=1961, end=1990):\n",
    "    \"\"\"\n",
    "    Climatologia 1961-1990 da temperatura média.\n",
    "    Se 'tmedia_mean' não existir (ou não houver anos dentro do intervalo),\n",
    "    devolve apenas a lista de estações com NaN — assim o merge não quebra.\n",
    "    \"\"\"\n",
    "    if 'tmedia_mean' not in seasonal.columns:\n",
    "        # devolve seasons únicas com NaN\n",
    "        return (seasonal[['season']]\n",
    "                .drop_duplicates()\n",
    "                .assign(clim_tmedia=np.nan))\n",
    "\n",
    "    base = seasonal.query(\"@start <= year <= @end\").dropna(subset=['tmedia_mean'])\n",
    "    if base.empty:\n",
    "        return (seasonal[['season']]\n",
    "                .drop_duplicates()\n",
    "                .assign(clim_tmedia=np.nan))\n",
    "\n",
    "    return (\n",
    "        base.groupby('season')\n",
    "            .agg(clim_tmedia=('tmedia_mean','mean'))\n",
    "            .reset_index()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff863f",
   "metadata": {},
   "source": [
    "## 4. Calcula agregados e anomalias para cada estação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca6a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando sazonais: 100%|██████████| 43/43 [00:00<00:00, 154.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# %% Calculo das métricas sazonais + anomalias\n",
    "seasonal_dict = {}\n",
    "anomaly_dict  = {}\n",
    "\n",
    "for code, df in tqdm(data_dict.items(), desc='Calculando sazonais'):\n",
    "    seas = seasonal_summary(df)                       # métricas da estação\n",
    "    clim = climatology_base(seas, BASELINE_START, BASELINE_END)\n",
    "\n",
    "    # junta climatologia (pode vir SEM 'clim_tmedia' se não houver tmedia)\n",
    "    seas = seas.merge(clim, on='season', how='left')\n",
    "\n",
    "    # calcula anomalia só se ambas existirem\n",
    "    if {'tmedia_mean', 'clim_tmedia'}.issubset(seas.columns):\n",
    "        seas['tmedia_anom'] = seas['tmedia_mean'] - seas['clim_tmedia']\n",
    "    else:\n",
    "        seas['tmedia_anom'] = np.nan                  # placeholder\n",
    "\n",
    "    seasonal_dict[code] = seas\n",
    "    anomaly_dict[code]  = seas[['year', 'season', 'tmedia_anom']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clima-rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
